# -*- coding: utf-8 -*-
"""Copy of CIS5190_Final_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PnNZvDtkq7NXl3EUCDm5q8knB4GX3zPp

# **Libraries**
"""

from urllib.parse import urlparse, urlencode, urljoin
import requests
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup
from google.colab import drive
from concurrent.futures import ThreadPoolExecutor, as_completed
from nltk.stem import PorterStemmer
import nltk
import time
import tensorflow as tf
import keras
import os

!pip install geopy > delete.txt
!pip install datasets > delete.txt
!pip install torch torchvision datasets > delete.txt
!pip install huggingface_hub > delete.txt
!rm delete.txt

from huggingface_hub import notebook_login
# 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
notebook_login()

drive.mount('/content/drive')

"""# **Data Collection & Normalization**

## Data Collecting
"""

csv_file_path = "/content/drive/My Drive/CIS5190/url_only_data.csv"
df = pd.read_csv(csv_file_path)
urls = df['url'].tolist()
titles = []

def fetch_title(url):
    try:
        headers = {
            'User-Agent': (
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                'AppleWebKit/537.36 (KHTML, like Gecko) '
                'Chrome/115.0.0.0 Safari/115.0.0'
            ),
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept': (
                'text/html,application/xhtml+xml,application/xml;'
                'q=0.9,image/webp,*/*;q=0.8'
            ),
            'Referer': 'https://www.google.com',
        }
        response = requests.get(url, timeout=10, headers=headers)

        if response.status_code != 200:
            raise Exception(f"Failed to load page: Status code {response.status_code}")

        # Ensure correct encoding
        response.encoding = response.apparent_encoding
        content = response.text

        # Parse the HTML content with BeautifulSoup
        soup = BeautifulSoup(content, "html.parser")

        # Find the news company
        news_company = urlparse(url).hostname

        # Find the title based on its tag
        title = soup.find("h1")

        if title:
            title_text = title.get_text(strip=True)
            return {"title": title_text, "news_company": news_company}
        else:
            print(f"No title found for URL: {url}")
            return None
    except Exception as e:
        print(f"Error processing URL {url}: {e}")
        return None

# Use ThreadPoolExecutor to parallelize requests
with ThreadPoolExecutor(max_workers=10) as executor:
    future_to_url = {executor.submit(fetch_title, url): url for url in urls}

    for future in as_completed(future_to_url):
        result = future.result()
        if result:
            titles.append(result)

print("Titles fetched:", titles)

titles_df = pd.DataFrame(titles)
titles_df.to_csv('/content/drive/My Drive/CIS5190/url_company.csv', index=False)

"""## Data Processing

### Add More Data
"""

csv_file_path = "/content/drive/My Drive/CIS5190/url_company.csv"
df = pd.read_csv(csv_file_path)
data = [{'title': row['title'], 'news_company': row['news_company']} for index, row in df.iterrows()]

# Adding missing ones
data.extend([
    {
        'title': "S.C. man sentenced to life in prison for murdering Black trans woman after historic verdict",
        'news_company': "www.nbcnews.com"
    },
    {
        'title': "Coalition to March on the DNC’s protest ends peacefully where it began",
        'news_company': "www.nbcnews.com"
    },
    {
        'title': "Instagram will use 'nudges' to start suggesting teens log off at night",
        'news_company': "www.nbcnews.com"
    },
    {
        'title': "Highlights and analysis: Trump commits to 'orderly transition' after mob storms Capitol",
        'news_company': "www.nbcnews.com"
    }
])

data_df = pd.DataFrame(data)
data_df.to_csv('/content/drive/My Drive/CIS5190/title_company_augmented.csv', index=False)

"""### Normalization"""

# Initialize the stemmer
stemmer = PorterStemmer()

# Work on the new dataset
csv_file_path = "/content/drive/My Drive/CIS5190/title_company_augmented.csv"
df = pd.read_csv(csv_file_path)
data = [{'title': row['title'], 'news_company': row['news_company']} for index, row in df.iterrows()]

# Normalize the data
for entry in data:
    # Convert the title to lowercase
    entry['title'] = entry['title'].lower()
    # Apply stemming to each word in the title
    entry['title'] = ' '.join([stemmer.stem(word) for word in entry['title'].split()])
    # Standardize the news company names (0 for NBC, 1 for FOX)
    if entry['news_company'] == "www.nbcnews.com" or entry['news_company'] == "nbcnews.com" or entry['news_company'] == "nbcnews":
        entry['news_company'] = 0
    elif entry['news_company'] == "www.foxnews.com" or entry['news_company'] == "foxnews.com" or entry['news_company'] == "foxnews":
        entry['news_company'] = 1

data_df = pd.DataFrame(data)
data_df.to_csv('/content/drive/My Drive/CIS5190/title_company_normalized.csv', index=False)

"""# **Model Training**

## Preprocess
"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold

csv_file_path = "/content/drive/My Drive/title_company_normalized.csv"
df = pd.read_csv(csv_file_path)

def train_test_gen(df):
  X = df['title']  # Features (text data)
  y = df['news_company']  # Labels (target variable)

  # Split the data into training and testing sets
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

  return X_train, X_test, y_train, y_test

def train_val_test_gen(df):
    X = df['title']  # Features (text data)
    y = df['news_company']  # Labels (target variable)

    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.15, random_state=42)

    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)

    return X_train, X_val, X_test, y_train, y_val, y_test

"""## Base Model"""

X_train, X_test, y_train, y_test = train_test_gen(df)

# Convert the text data to TF-IDF features
vectorizer = TfidfVectorizer(stop_words='english', max_features=100)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Train a Logistic Regression model
model = LogisticRegression(max_iter=100)
model.fit(X_train_tfidf, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test_tfidf)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")
print("Classification Report:\n", classification_report(y_test, y_pred))

"""## Improvement Section 1 (TF-IDF)"""

X_train, X_test, y_train, y_test = train_test_gen(df)

# Convert the text data to TF-IDF features
vectorizer = TfidfVectorizer(stop_words='english') # no feature limit
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Train a Logistic Regression model
model = LogisticRegression(max_iter=100)
model.fit(X_train_tfidf, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test_tfidf)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")
print("Classification Report:\n", classification_report(y_test, y_pred))

X_train, X_test, y_train, y_test = train_test_gen(df)

# Convert the text data to TF-IDF features
vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 3))
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Train a Logistic Regression model
model = LogisticRegression(max_iter=100)
model.fit(X_train_tfidf, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test_tfidf)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")
print("Classification Report:\n", classification_report(y_test, y_pred))

"""## Improvement Section 2 (Different Basic Models)"""

from sklearn.svm import LinearSVC

X_train, X_test, y_train, y_test = train_test_gen(df)

# Convert the text data to TF-IDF features
vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 3))
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Train a Linear SVC model
model = LinearSVC()
model.fit(X_train_tfidf, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test_tfidf)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")
print("Classification Report:\n", classification_report(y_test, y_pred))

from sklearn.ensemble import RandomForestClassifier

X_train, X_test, y_train, y_test = train_test_gen(df)

# Convert the text data to TF-IDF features
vectorizer = TfidfVectorizer(stop_words='english')
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Train a Random Forest model
model = RandomForestClassifier(n_estimators=100)
model.fit(X_train_tfidf, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test_tfidf)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")
print("Classification Report:\n", classification_report(y_test, y_pred))

from sklearn.naive_bayes import MultinomialNB

X_train, X_test, y_train, y_test = train_test_gen(df)

# Convert the text data to TF-IDF features
vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 3))
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Train a Multinomial Naive Bayes model
model = MultinomialNB()
model.fit(X_train_tfidf, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test_tfidf)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")
print("Classification Report:\n", classification_report(y_test, y_pred))

"""**KNN**"""

from sklearn.neighbors import KNeighborsClassifier

X_train, X_test, y_train, y_test = train_test_gen(df)

# Convert the text data to TF-IDF features
vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 3))
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Train a KNN model
model = KNeighborsClassifier(n_neighbors=5)
model.fit(X_train_tfidf, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test_tfidf)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")
print("Classification Report:\n", classification_report(y_test, y_pred))

"""**Neural Network**"""

# initial model | accuracy: 0.8200
from keras.models import Sequential
from keras.layers import Dense, Dropout

# Convert the text data to TF-IDF features
vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 3))
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Feed-forward
model = Sequential()
model.add(Dense(512, input_dim=X_train_tfidf.shape[1], activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.fit(X_train_tfidf, y_train, epochs=10, batch_size=32, validation_data=(X_test_tfidf, y_test))
y_pred = (model.predict(X_test_tfidf) > 0.5).astype(int)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")
print("Classification Report:\n", classification_report(y_test, y_pred))

# adjust initial parameters
from keras.models import Sequential
from keras.layers import Dense, Dropout

X_train, X_test, y_train, y_test = train_test_gen(df)

# Convert the text data to TF-IDF features
vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 3))
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Feed-forward
model = Sequential()
model.add(Dense(1024, input_dim=X_train_tfidf.shape[1], activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.fit(X_train_tfidf, y_train, epochs=10, batch_size=32, validation_data=(X_test_tfidf, y_test))
y_pred = (model.predict(X_test_tfidf) > 0.5).astype(int)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")
print("Classification Report:\n", classification_report(y_test, y_pred))

# add one dense layer
from keras.models import Sequential
from keras.layers import Dense, Dropout

X_train, X_test, y_train, y_test = train_test_gen(df)

# Convert the text data to TF-IDF features
vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 3))
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Feed-forward
model = Sequential()
model.add(Dense(1024, input_dim=X_train_tfidf.shape[1], activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.fit(X_train_tfidf, y_train, epochs=10, batch_size=32, validation_data=(X_test_tfidf, y_test))
y_pred = (model.predict(X_test_tfidf) > 0.5).astype(int)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")
print("Classification Report:\n", classification_report(y_test, y_pred))

# adam with decling rate (hyperparameter tunning)
from keras.models import Sequential
from keras.layers import Dense, Dropout

X_train, X_test, y_train, y_test = train_test_gen(df)

# Convert the text data to TF-IDF features
vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 3))
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Feed-forward
model = Sequential()
model.add(Dense(1024, input_dim=X_train_tfidf.shape[1], activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))
optimizer = keras.optimizers.Adam(learning_rate=0.001)
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])
model.fit(X_train_tfidf, y_train, epochs=10, batch_size=32, validation_data=(X_test_tfidf, y_test))
y_pred = (model.predict(X_test_tfidf) > 0.5).astype(int)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")
print("Classification Report:\n", classification_report(y_test, y_pred))

#没事别按
model.save("hf://CIS5190kr/News_Resource_Classification")

"""**LSTM**"""

from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import Embedding, LSTM, Bidirectional
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical

X_train, X_test, y_train, y_test = train_test_gen(df)


tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')
tokenizer.fit_on_texts(X_train)

X_train_seq = tokenizer.texts_to_sequences(X_train)
X_test_seq = tokenizer.texts_to_sequences(X_test)

max_length = 50
X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post', truncating='post')
X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post', truncating='post')

model = Sequential([
    Embedding(input_dim=10000, output_dim=128, input_length=max_length),
    Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)),
    # Bidirectional(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)),
    # Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.fit(X_train_pad, y_train, epochs=10, batch_size=32, validation_data=(X_test_pad, y_test))

loss, accuracy = model.evaluate(X_test_pad, y_test)
print(f"Accuracy: {accuracy:.4f}")

y_pred = (model.predict(X_test_pad) > 0.5).astype(int)
print("Classification Report:\n", classification_report(y_test, y_pred))

# using Word2Vec
from gensim.models import Word2Vec
X_train, X_test, y_train, y_test = train_test_gen(df)

word2vec_model = Word2Vec(sentences=X_train, vector_size=100, window=5, min_count=1, workers=4)
word2vec_model.save("word2vec.model")

vocab_size = len(word2vec_model.wv.index_to_key)
print(f"Vocabulary Size: {vocab_size}")

embedding_dim = 100
embedding_matrix = np.zeros((vocab_size + 1, embedding_dim))

for i, word in enumerate(word2vec_model.wv.index_to_key):
    embedding_matrix[i + 1] = word2vec_model.wv[word]


tokenizer = Tokenizer(num_words=vocab_size)
tokenizer.fit_on_texts(X_train)

X_train_seq = tokenizer.texts_to_sequences(X_train)
X_test_seq = tokenizer.texts_to_sequences(X_test)

max_length = 50
X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post', truncating='post')
X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post', truncating='post')

model = Sequential([
    Embedding(input_dim=vocab_size + 1, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False),
    Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.fit(X_train_pad, y_train, epochs=10, batch_size=32, validation_data=(X_test_pad, y_test))

loss, accuracy = model.evaluate(X_test_pad, y_test)
print(f"Accuracy: {accuracy:.4f}")

y_pred = (model.predict(X_test_pad) > 0.5).astype(int)
print("Classification Report:\n", classification_report(y_test, y_pred))

"""**Transformer**"""

from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import Embedding, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras import Input

X_train, X_test, y_train, y_test = train_test_gen(df)

tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')
tokenizer.fit_on_texts(X_train)

X_train_seq = tokenizer.texts_to_sequences(X_train)
X_test_seq = tokenizer.texts_to_sequences(X_test)

max_length = 50
X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post', truncating='post')
X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post', truncating='post')

class TransformerBlock(Model):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):
        super(TransformerBlock, self).__init__()
        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = Sequential([
            Dense(ff_dim, activation="relu"),
            Dense(embed_dim),
        ])
        self.layernorm1 = LayerNormalization(epsilon=1e-6)
        self.layernorm2 = LayerNormalization(epsilon=1e-6)
        self.dropout1 = Dropout(rate)
        self.dropout2 = Dropout(rate)

    def call(self, inputs, training):
        attn_output = self.att(inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)


embed_dim = 128
num_heads = 4
ff_dim = 128

inputs = Input(shape=(max_length,))
embedding_layer = Embedding(input_dim=10000, output_dim=embed_dim, input_length=max_length)(inputs)
transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)(embedding_layer, training=True)

pooling = GlobalAveragePooling1D()(transformer_block)
dense = Dense(64, activation="relu")(pooling)
dropout = Dropout(0.5)(dense)
outputs = Dense(1, activation="sigmoid")(dropout)

model = Model(inputs=inputs, outputs=outputs)
model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])
model.fit(X_train_pad, y_train, batch_size=32, epochs=10, validation_data=(X_test_pad, y_test))
loss, accuracy = model.evaluate(X_test_pad, y_test)
print(f"Accuracy: {accuracy:.4f}")
y_pred = (model.predict(X_test_pad) > 0.5).astype(int)
print("Classification Report:\n", classification_report(y_test, y_pred))

"""***Transformer with BERT***"""

# bert
import tensorflow as tf
from transformers import BertTokenizer, TFBertForSequenceClassification, create_optimizer
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import AdamWeightDecay # Import AdamWeightDecay instead of Keras Adam
# from tensorflow.keras.optimizers import Adam

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
X_train, X_test, y_train, y_test = train_test_gen(df)

# Tokenize the text data
def tokenize_data(texts, tokenizer, max_length=128):
    return tokenizer(
        list(texts),
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_tensors="tf"
    )

train_encodings = tokenize_data(X_train, tokenizer)
test_encodings = tokenize_data(X_test, tokenizer)

# Convert to TensorFlow datasets
train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(train_encodings),
    list(y_train)  # Convert labels to list if not already
)).shuffle(len(y_train)).batch(16)

test_dataset = tf.data.Dataset.from_tensor_slices((
    dict(test_encodings),
    list(y_test)  # Convert labels to list if not already
)).batch(16)

# Load the pre-trained BERT model for sequence classification
bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)


# Compile the model
optimizer = AdamWeightDecay(learning_rate=5e-5) # Use AdamWeightDecay

loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
bert_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])

# Train the model
history = bert_model.fit(
    train_dataset,
    validation_data=test_dataset,
    epochs=10
)

# Evaluate the model

y_pred = tf.argmax(bert_model.predict(test_dataset)[0], axis=1).numpy()
accuracy = accuracy_score(list(y_test), y_pred)
print(f"Accuracy: {accuracy:.4f}")
print(classification_report(list(y_test), y_pred, target_names=['NBC', 'Fox']))

bert_model.summary()

# roberta
import tensorflow as tf
from transformers import RobertaTokenizer, TFRobertaForSequenceClassification
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import AdamWeightDecay # Import AdamWeightDecay instead of Keras Adam
# from tensorflow.keras.optimizers import Adam

tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
X_train, X_test, y_train, y_test = train_test_gen(df)

# Tokenize the text data
def tokenize_data(texts, tokenizer, max_length=128):
    return tokenizer(
        list(texts),
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_tensors="tf"
    )

train_encodings = tokenize_data(X_train, tokenizer)
test_encodings = tokenize_data(X_test, tokenizer)

# Convert to TensorFlow datasets
train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(train_encodings),
    list(y_train)  # Convert labels to list if not already
)).shuffle(len(y_train)).batch(16)

test_dataset = tf.data.Dataset.from_tensor_slices((
    dict(test_encodings),
    list(y_test)  # Convert labels to list if not already
)).batch(16)

# Load the pre-trained BERT model for sequence classification
roberta_model = TFRobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)



# Compile the model
optimizer = AdamWeightDecay(learning_rate=5e-5) # Use AdamWeightDecay

loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
roberta_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])

# Train the model
history = roberta_model.fit(
    train_dataset,
    validation_data=test_dataset,
    epochs=10
)

# Evaluate the model
y_pred = tf.argmax(roberta_model.predict(test_dataset)[0], axis=1).numpy()
accuracy = accuracy_score(list(y_test), y_pred)
print(f"Accuracy: {accuracy:.4f}")
print(classification_report(list(y_test), y_pred, target_names=['NBC', 'Fox']))

# distilbert
import tensorflow as tf
from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import AdamWeightDecay # Import AdamWeightDecay instead of Keras Adam
# from tensorflow.keras.optimizers import Adam

tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
X_train, X_test, y_train, y_test = train_test_gen(df)

# Tokenize the text data
def tokenize_data(texts, tokenizer, max_length=128):
    return tokenizer(
        list(texts),
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_tensors="tf"
    )

train_encodings = tokenize_data(X_train, tokenizer)
test_encodings = tokenize_data(X_test, tokenizer)

# Convert to TensorFlow datasets
train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(train_encodings),
    list(y_train)  # Convert labels to list if not already
)).shuffle(len(y_train)).batch(16)

test_dataset = tf.data.Dataset.from_tensor_slices((
    dict(test_encodings),
    list(y_test)  # Convert labels to list if not already
)).batch(16)

# Load the pre-trained BERT model for sequence classification
distilbert_model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)


# Compile the model
optimizer = AdamWeightDecay(learning_rate=5e-5) # Use AdamWeightDecay

loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
distilbert_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])

# Train the model
history = distilbert_model.fit(
    train_dataset,
    validation_data=test_dataset,
    epochs=10
)

# Evaluate the model

y_pred = tf.argmax(bert_model.predict(test_dataset)[0], axis=1).numpy()
accuracy = accuracy_score(list(y_test), y_pred)
print(f"Accuracy: {accuracy:.4f}")
print(classification_report(list(y_test), y_pred, target_names=['NBC', 'Fox']))

"""# ***Add Dense Layer***"""

import tensorflow as tf
from transformers import BertTokenizer, TFBertModel
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import Dense, Dropout, Input
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import classification_report
import pandas as pd


tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
bert_main = bert_model.bert
X_train, X_test, y_train, y_test = train_test_gen(df)

# Tokenize the text data
def tokenize_data(texts, tokenizer, max_length=128):
    return tokenizer(
        list(texts),
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_tensors="tf"
    )

train_encodings = tokenize_data(X_train, tokenizer)
test_encodings = tokenize_data(X_test, tokenizer)

# Convert to TensorFlow datasets
train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(train_encodings),
    list(y_train)  # Convert labels to list if not already
)).shuffle(len(y_train)).batch(16)

test_dataset = tf.data.Dataset.from_tensor_slices((
    dict(test_encodings),
    list(y_test)  # Convert labels to list if not already
)).batch(16)


input_ids = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name="input_ids")
attention_mask = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name="attention_mask")

# Get BERT outputs directly
bert_outputs = bert_main({"input_ids": input_ids, "attention_mask": attention_mask})
pooled_output = bert_outputs.pooler_output  # [CLS] token 的池化输出


# Apply Dense and Dropout layers to the pooled output
x = layers.Dense(256, activation='relu')(pooled_output)
x = layers.Dropout(0.3)(x)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.3)(x)
output = layers.Dense(2, activation='softmax')(x)  # Output layer

# Define the Keras model
model = Model(inputs=[input_ids, attention_mask], outputs=output)

# Compile the model
optimizer = AdamWeightDecay(learning_rate=5e-5)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)
model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])

# Train the model
history = model.fit(
    train_dataset,
    validation_data=test_dataset,
    epochs=5,
    batch_size=16
)

# Model evaluation
y_pred = tf.argmax(model.predict(test_dataset), axis=1).numpy()
print(classification_report(list(y_test), y_pred, target_names=['Fox', 'NBC']))

# Print accuracy
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(list(y_test), y_pred)
print(f"Accuracy: {accuracy:.4f}")

"""#***NN Hyperparameters Tunning***"""

# Feed Forward Neural Network | Accuracy: 0.8265
from keras.models import Sequential
from keras.layers import Dense, Dropout

X_train, X_test, y_train, y_test = train_test_gen(df)

# Convert the text data to TF-IDF features
vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 3))
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Feed-forward
model = Sequential()
model.add(Dense(1024, input_dim=X_train_tfidf.shape[1], activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.fit(X_train_tfidf, y_train, epochs=10, batch_size=32, validation_data=(X_test_tfidf, y_test))
y_pred = (model.predict(X_test_tfidf) > 0.5).astype(int)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")
print("Classification Report:\n", classification_report(y_test, y_pred))

"""Grid Search"""

# grid search
!pip install scikeras
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from scikeras.wrappers import KerasClassifier
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense, Dropout

X_train, X_test, y_train, y_test = train_test_gen(df)

# Convert the text data to TF-IDF features
vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 3))
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# convert our model to an object class
def best_model(learning_rate=0.001, dropout=0.5, first_layer=1024, second_layer=256):
    model = Sequential()
    model.add(Dense(first_layer, input_dim=X_train_tfidf.shape[1], activation='relu'))
    model.add(Dropout(dropout))
    model.add(Dense(second_layer, activation='relu'))
    model.add(Dropout(dropout))
    model.add(Dense(1, activation='sigmoid'))
    # model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])
    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])
    return model


# define hyperparamters' range
param_grid = {
    # 'model__learning_rate': [0.001, 0.01],
    'model__dropout': [0.3, 0.5],
    'model__first_layer': [512, 1024],
    'model__second_layer': [128, 256],
    # 'batch_size': [32, 64],
    'epochs': [10, 20]
}

# convert the model to sklearn
model = KerasClassifier(build_fn=best_model, verbose=0)

# GridSearchCV
grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy', cv=3)
grid_result = grid.fit(X_train_tfidf.toarray(), y_train)

print(f"Best Parameters: {grid_result.best_params_}")
print(f"Best Accuracy: {grid_result.best_score_}")

"""Random Search"""

from sklearn.model_selection import RandomizedSearchCV

# RandomizedSearchCV
random_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=20, scoring='accuracy', cv=3, random_state=42)
random_result = random_search.fit(X_train_tfidf.toarray(), y_train)

print(f"Best Parameters: {random_result.best_params_}")
print(f"Best Accuracy: {random_result.best_score_}")

"""# Bert Tuning"""

# bert with validation
import tensorflow as tf
from transformers import BertTokenizer, TFBertForSequenceClassification, create_optimizer
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import AdamWeightDecay # Import AdamWeightDecay instead of Keras Adam
# from tensorflow.keras.optimizers import Adam

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
X_train, X_validation, X_test, y_train, y_validation, y_test = train_val_test_gen(df)

# Tokenize the text data
def tokenize_data(texts, tokenizer, max_length=128):
    return tokenizer(
        list(texts),
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_tensors="tf"
    )
train_encodings = tokenize_data(X_train, tokenizer)
validation_encodings = tokenize_data(X_validation, tokenizer)  # 验证集
test_encodings = tokenize_data(X_test, tokenizer)  # 测试集

# Convert to TensorFlow datasets
train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(train_encodings),
    list(y_train)  # Convert labels to list if not already
)).shuffle(len(y_train)).batch(16)

test_dataset = tf.data.Dataset.from_tensor_slices((
    dict(test_encodings),
    list(y_test)  # Convert labels to list if not already
)).batch(16)

validation_dataset = tf.data.Dataset.from_tensor_slices((
    dict(validation_encodings),
    list(y_validation)  # 转为列表
)).batch(16)

# Load the pre-trained BERT model for sequence classification
bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)


# Compile the model
optimizer = AdamWeightDecay(learning_rate=5e-5) # Use AdamWeightDecay

loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
bert_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])

# Train the model
history = bert_model.fit(
    train_dataset,
    validation_data=validation_dataset,  # 使用验证集
    epochs=3
)

# Evaluate the model

y_pred = tf.argmax(bert_model.predict(test_dataset)[0], axis=1).numpy()
accuracy = accuracy_score(list(y_test), y_pred)
print(f"Test Accuracy: {accuracy:.4f}")
print(classification_report(list(y_test), y_pred, target_names=['NBC', 'Fox']))

"""***k fold***"""

import tensorflow as tf
from transformers import BertTokenizer, TFBertForSequenceClassification, create_optimizer
from transformers import AdamWeightDecay
from sklearn.model_selection import KFold
from sklearn.metrics import classification_report, accuracy_score
import numpy as np

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
X = df['title']  # Features (text data)
y = df['news_company']  # Labels (target variable)

def tokenize_data(texts, tokenizer, max_length=128):
    return tokenizer(
        list(texts),
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_tensors="tf"
    )

kf = KFold(n_splits=5, shuffle=True, random_state=42)
fold = 1
fold_accuracies = []

for train_index, test_index in kf.split(X):
    print(f"\nFold {fold}")
    # 分割训练集和测试集
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y[train_index], y[test_index]

    train_encodings = tokenize_data(X_train, tokenizer)
    test_encodings = tokenize_data(X_test, tokenizer)

    train_dataset = tf.data.Dataset.from_tensor_slices((
        dict(train_encodings),
        list(y_train)
    )).shuffle(len(y_train)).batch(16)

    test_dataset = tf.data.Dataset.from_tensor_slices((
        dict(test_encodings),
        list(y_test)
    )).batch(16)

    bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

    # compile model
    optimizer = AdamWeightDecay(learning_rate=5e-5)
    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
    bert_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])

    bert_model.fit(
        train_dataset,
        validation_data=test_dataset,
        epochs=3
    )

    y_pred = tf.argmax(bert_model.predict(test_dataset)[0], axis=1).numpy()
    accuracy = accuracy_score(list(y_test), y_pred)
    fold_accuracies.append(accuracy)

    print(f"Fold {fold} Accuracy: {accuracy:.4f}")
    print(classification_report(list(y_test), y_pred, target_names=['NBC', 'Fox']))

    fold += 1

# 输出平均性能
print("\nK-Fold Cross Validation Results")
print(f"Mean Accuracy: {np.mean(fold_accuracies):.4f}")
print(f"Standard Deviation: {np.std(fold_accuracies):.4f}")

"""bert k fold with validation data"""

import tensorflow as tf
from transformers import BertTokenizer, TFBertForSequenceClassification, AdamWeightDecay
from sklearn.model_selection import KFold, train_test_split
from sklearn.metrics import classification_report, accuracy_score
import numpy as np

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
X = df['title']  # Features (text data)
y = df['news_company']  # Labels (target variable)

def tokenize_data(texts, tokenizer, max_length=128):
    return tokenizer(
        list(texts),
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_tensors="tf"
    )

kf = KFold(n_splits=5, shuffle=True, random_state=42)
fold = 1
fold_accuracies = []

for train_index, test_index in kf.split(X):
    print(f"\nFold {fold}")
    # 分割训练集和测试集
    X_train_full, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train_full, y_test = y.iloc[train_index], y.iloc[test_index]

    # 从训练集进一步划分验证集
    X_train, X_val, y_train, y_val = train_test_split(
        X_train_full, y_train_full, test_size=0.2, random_state=42
    )

    # 编码数据
    train_encodings = tokenize_data(X_train, tokenizer)
    val_encodings = tokenize_data(X_val, tokenizer)
    test_encodings = tokenize_data(X_test, tokenizer)

    # 构建 TensorFlow 数据集
    train_dataset = tf.data.Dataset.from_tensor_slices((
        dict(train_encodings),
        list(y_train)
    )).shuffle(len(y_train)).batch(16)

    val_dataset = tf.data.Dataset.from_tensor_slices((
        dict(val_encodings),
        list(y_val)
    )).batch(16)

    test_dataset = tf.data.Dataset.from_tensor_slices((
        dict(test_encodings),
        list(y_test)
    )).batch(16)

    # 加载预训练 BERT 模型
    bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

    # 编译模型
    optimizer = AdamWeightDecay(learning_rate=5e-5)
    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
    bert_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])

    # 训练模型
    bert_model.fit(
        train_dataset,
        validation_data=val_dataset,
        epochs=3
    )

    # 测试集预测
    y_pred = tf.argmax(bert_model.predict(test_dataset)[0], axis=1).numpy()
    accuracy = accuracy_score(list(y_test), y_pred)
    fold_accuracies.append(accuracy)

    print(f"Fold {fold} Accuracy: {accuracy:.4f}")
    print(classification_report(list(y_test), y_pred, target_names=['NBC', 'Fox']))

    fold += 1

# 输出平均性能
print("\nK-Fold Cross Validation Results")
print(f"Mean Accuracy: {np.mean(fold_accuracies):.4f}")
print(f"Standard Deviation: {np.std(fold_accuracies):.4f}")

"""***Hyperparamter Tuning***"""

# bert - grid search
import tensorflow as tf
from transformers import BertTokenizer, TFBertForSequenceClassification, create_optimizer
from transformers import AdamWeightDecay
from sklearn.metrics import classification_report, accuracy_score
from sklearn.model_selection import train_test_split
import itertools

# 预处理
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

def tokenize_data(texts, tokenizer, max_length=128):
    return tokenizer(
        list(texts),
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_tensors="tf"
    )

X_train, X_test, y_train, y_test = train_test_gen(df)

train_encodings = tokenize_data(X_train, tokenizer)
test_encodings = tokenize_data(X_test, tokenizer)

train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(train_encodings),
    list(y_train)
)).shuffle(len(y_train)).batch(16)

test_dataset = tf.data.Dataset.from_tensor_slices((
    dict(test_encodings),
    list(y_test)
)).batch(16)

# 超参数字典
param_grid = {
    'dropout_rate': [0.1, 0.5],
    'epochs': [3, 5, 6, 7],
    'learning_rate': [3e-5, 5e-5]
}

# Grid Search
best_model = None
best_accuracy = 0
best_params = None

# 遍历所有可能的超参数组合
for params in itertools.product(*param_grid.values()):
    current_params = dict(zip(param_grid.keys(), params))
    print(f"Testing params: {current_params}")

    # 创建模型
    bert_model = TFBertForSequenceClassification.from_pretrained(
        'bert-base-uncased',
        num_labels=2,
        hidden_dropout_prob=current_params['dropout_rate']
    )

    optimizer = AdamWeightDecay(learning_rate=current_params['learning_rate'])
    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
    bert_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])

    # EarlyStopping 回调
    early_stopping = tf.keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=10,  # 提前停止的轮次
        restore_best_weights=True
    )

    # 训练模型
    history = bert_model.fit(
        train_dataset,
        validation_data=test_dataset,
        epochs=current_params['epochs'],
        callbacks=[early_stopping]
    )

    # 模型评估
    y_pred = tf.argmax(bert_model.predict(test_dataset)[0], axis=1).numpy()
    accuracy = accuracy_score(list(y_test), y_pred)
    print(f"Accuracy: {accuracy:.4f}")

    if accuracy > best_accuracy:
        best_accuracy = accuracy
        best_model = bert_model
        best_params = current_params

print(f"Best Accuracy: {best_accuracy:.4f}")
print(f"Best Params: {best_params}")

# 打印 Best Model 的 Classification Report
if best_model:
    y_pred_best = tf.argmax(best_model.predict(test_dataset)[0], axis=1).numpy()
    print("\nClassification Report for Best Model:")
    print(classification_report(list(y_test), y_pred_best, target_names=['NBC', 'Fox']))

# bert - grid search with early stopping
import tensorflow as tf
from transformers import BertTokenizer, TFBertForSequenceClassification, create_optimizer
from transformers import AdamWeightDecay
from sklearn.metrics import classification_report, accuracy_score
from sklearn.model_selection import train_test_split
import itertools

# 预处理
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

def tokenize_data(texts, tokenizer, max_length=128):
    return tokenizer(
        list(texts),
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_tensors="tf"
    )

# 生成训练集、验证集和测试集
X_train, X_validation, X_test, y_train, y_validation, y_test = train_val_test_gen(df)

# 对训练集、验证集和测试集进行 Tokenization
train_encodings = tokenize_data(X_train, tokenizer)
validation_encodings = tokenize_data(X_validation, tokenizer)
test_encodings = tokenize_data(X_test, tokenizer)

# 创建 TensorFlow Dataset
train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(train_encodings),
    list(y_train)
)).shuffle(len(y_train)).batch(16)

validation_dataset = tf.data.Dataset.from_tensor_slices((
    dict(validation_encodings),
    list(y_validation)
)).batch(16)

test_dataset = tf.data.Dataset.from_tensor_slices((
    dict(test_encodings),
    list(y_test)
)).batch(16)

# 超参数字典
param_grid = {
    'dropout_rate': [0.1, 0.2, 0.3],
    'epochs': [3, 5, 6, 7],
    'learning_rate': [5e-5]
}

# Grid Search
best_model = None
best_accuracy = 0
best_params = None

# 遍历所有可能的超参数组合
for params in itertools.product(*param_grid.values()):
    current_params = dict(zip(param_grid.keys(), params))
    print(f"Testing params: {current_params}")

    # 创建模型
    bert_model = TFBertForSequenceClassification.from_pretrained(
        'bert-base-uncased',
        num_labels=2,
        hidden_dropout_prob=current_params['dropout_rate']
    )

    optimizer = AdamWeightDecay(learning_rate=current_params['learning_rate'])
    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
    bert_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])

    # EarlyStopping 回调
    early_stopping = tf.keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=10,  # 提前停止的轮次
        restore_best_weights=True
    )

    # 训练模型
    history = bert_model.fit(
        train_dataset,
        validation_data=validation_dataset,
        epochs=current_params['epochs'],
        callbacks=[early_stopping]
    )

    # 模型评估
    y_pred = tf.argmax(bert_model.predict(test_dataset)[0], axis=1).numpy()
    accuracy = accuracy_score(list(y_test), y_pred)
    print(f"Accuracy: {accuracy:.4f}")

    if accuracy > best_accuracy:
        best_accuracy = accuracy
        best_model = bert_model
        best_params = current_params

print(f"Best Accuracy: {best_accuracy:.4f}")
print(f"Best Params: {best_params}")

# 打印 Best Model 的 Classification Report
if best_model:
    y_pred_best = tf.argmax(best_model.predict(test_dataset)[0], axis=1).numpy()
    print("\nClassification Report for Best Model:")
    print(classification_report(list(y_test), y_pred_best, target_names=['NBC', 'Fox']))

# bert-random search
import tensorflow as tf
from transformers import BertTokenizer, TFBertForSequenceClassification, create_optimizer
from transformers import AdamWeightDecay
from sklearn.metrics import classification_report, accuracy_score
from sklearn.model_selection import train_test_split
import itertools
import random

# 预处理
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
X_train, X_test, y_train, y_test = train_test_gen(df)

def tokenize_data(texts, tokenizer, max_length=128):
    return tokenizer(
        list(texts),
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_tensors="tf"
    )

train_encodings = tokenize_data(X_train, tokenizer)
test_encodings = tokenize_data(X_test, tokenizer)

train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(train_encodings),
    list(y_train)
)).shuffle(len(y_train)).batch(16)

test_dataset = tf.data.Dataset.from_tensor_slices((
    dict(test_encodings),
    list(y_test)
)).batch(16)

param_grid = {
    'dropout_rate': [0.1, 0.2, 0.3],
    'epochs': [3, 5, 7],
    'learning_rate': [2e-5, 3e-5, 5e-5]
}

# 随机搜索样本数量
random_search_samples = 5

def random_search(grid, n_samples):
    keys = list(grid.keys())
    samples = [dict(zip(keys, values)) for values in random.sample(list(itertools.product(*grid.values())), n_samples)]
    return samples

# EarlyStopping 回调
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=10,  # 提前停止的轮次
    restore_best_weights=True
)

# 模型训练和验证
best_model = None
best_accuracy = 0
best_params = None

for params in random_search(param_grid, random_search_samples):
    print(f"Testing params: {params}")

    bert_model = TFBertForSequenceClassification.from_pretrained(
        'bert-base-uncased',
        num_labels=2,
        hidden_dropout_prob=params['dropout_rate']
    )

    optimizer = AdamWeightDecay(learning_rate=params['learning_rate'])
    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
    bert_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])

    history = bert_model.fit(
        train_dataset,
        validation_data=test_dataset,
        epochs=params['epochs'],
        callbacks=[early_stopping]
    )

    y_pred = tf.argmax(bert_model.predict(test_dataset)[0], axis=1).numpy()
    accuracy = accuracy_score(list(y_test), y_pred)
    print(f"Accuracy: {accuracy:.4f}")

    if accuracy > best_accuracy:
        best_accuracy = accuracy
        best_model = bert_model
        best_params = params

print(f"Best Accuracy: {best_accuracy:.4f}")
print(f"Best Params: {best_params}")

"""# ***DistilBert Tuning***"""

# distilbert with validation
import tensorflow as tf
from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import AdamWeightDecay # Import AdamWeightDecay instead of Keras Adam
# from tensorflow.keras.optimizers import Adam

tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
X_train, X_validation, X_test, y_train, y_validation, y_test = train_val_test_gen(df)

# Tokenize the text data
def tokenize_data(texts, tokenizer, max_length=128):
    return tokenizer(
        list(texts),
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_tensors="tf"
    )

train_encodings = tokenize_data(X_train, tokenizer)
test_encodings = tokenize_data(X_test, tokenizer)
validation_encodings = tokenize_data(X_validation, tokenizer)  # 验证集

# Convert to TensorFlow datasets
train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(train_encodings),
    list(y_train)  # Convert labels to list if not already
)).shuffle(len(y_train)).batch(16)

test_dataset = tf.data.Dataset.from_tensor_slices((
    dict(test_encodings),
    list(y_test)  # Convert labels to list if not already
)).batch(16)

validation_dataset = tf.data.Dataset.from_tensor_slices((
    dict(validation_encodings),
    list(y_validation)  # 转为列表
)).batch(16)

# Load the pre-trained distilbert model for sequence classification
distilbert_model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)


# Compile the model
optimizer = AdamWeightDecay(learning_rate=5e-5) # Use AdamWeightDecay

loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
distilbert_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])

# Train the model
history = distilbert_model.fit(
    train_dataset,
    validation_data=validation_dataset,
    epochs=10
)

# Evaluate the model

y_pred = tf.argmax(bert_model.predict(test_dataset)[0], axis=1).numpy()
accuracy = accuracy_score(list(y_test), y_pred)
print(f"Accuracy: {accuracy:.4f}")
print(classification_report(list(y_test), y_pred, target_names=['NBC', 'Fox']))

"""k fold"""

import tensorflow as tf
from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import AdamWeightDecay # Import AdamWeightDecay instead of Keras Adam
# from tensorflow.keras.optimizers import Adam

tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
X = df['title']  # Features (text data)
y = df['news_company']  # Labels (target variable)

def tokenize_data(texts, tokenizer, max_length=128):
    return tokenizer(
        list(texts),
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_tensors="tf"
    )

kf = KFold(n_splits=5, shuffle=True, random_state=42)
fold = 1
fold_accuracies = []

for train_index, test_index in kf.split(X):
    print(f"\nFold {fold}")
    # 分割训练集和测试集
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y[train_index], y[test_index]

    train_encodings = tokenize_data(X_train, tokenizer)
    test_encodings = tokenize_data(X_test, tokenizer)

    train_dataset = tf.data.Dataset.from_tensor_slices((
        dict(train_encodings),
        list(y_train)
    )).shuffle(len(y_train)).batch(16)

    test_dataset = tf.data.Dataset.from_tensor_slices((
        dict(test_encodings),
        list(y_test)
    )).batch(16)

    distilbert_model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)


    # compile model
    optimizer = AdamWeightDecay(learning_rate=5e-5)
    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
    distilbert_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])

    distilbert_model.fit(
        train_dataset,
        validation_data=test_dataset,
        epochs=3
    )

    y_pred = tf.argmax(distilbert_model.predict(test_dataset)[0], axis=1).numpy()
    accuracy = accuracy_score(list(y_test), y_pred)
    fold_accuracies.append(accuracy)

    print(f"Fold {fold} Accuracy: {accuracy:.4f}")
    print(classification_report(list(y_test), y_pred, target_names=['NBC', 'Fox']))

    fold += 1

# 输出平均性能
print("\nK-Fold Cross Validation Results")
print(f"Mean Accuracy: {np.mean(fold_accuracies):.4f}")
print(f"Standard Deviation: {np.std(fold_accuracies):.4f}")

"""***distil bert k-fold with validation dataset***"""

import tensorflow as tf
from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import AdamWeightDecay # Import AdamWeightDecay instead of Keras Adam
# from tensorflow.keras.optimizers import Adam

tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
X = df['title']  # Features (text data)
y = df['news_company']  # Labels (target variable)

def tokenize_data(texts, tokenizer, max_length=128):
    return tokenizer(
        list(texts),
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_tensors="tf"
    )

kf = KFold(n_splits=5, shuffle=True, random_state=42)
fold = 1
fold_accuracies = []

for train_index, test_index in kf.split(X):
    print(f"\nFold {fold}")
    # 分割训练集和测试集
    X_train_full, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train_full, y_test = y.iloc[train_index], y.iloc[test_index]

    # 从训练集进一步划分验证集
    X_train, X_val, y_train, y_val = train_test_split(
        X_train_full, y_train_full, test_size=0.2, random_state=42
    )

    val_encodings = tokenize_data(X_val, tokenizer)
    train_encodings = tokenize_data(X_train, tokenizer)
    test_encodings = tokenize_data(X_test, tokenizer)

    train_dataset = tf.data.Dataset.from_tensor_slices((
        dict(train_encodings),
        list(y_train)
    )).shuffle(len(y_train)).batch(16)

    test_dataset = tf.data.Dataset.from_tensor_slices((
        dict(test_encodings),
        list(y_test)
    )).batch(16)

    val_dataset = tf.data.Dataset.from_tensor_slices((
        dict(val_encodings),
        list(y_val)
    )).batch(16)

    distilbert_model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)


    # compile model
    optimizer = AdamWeightDecay(learning_rate=5e-5)
    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
    distilbert_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])

    distilbert_model.fit(
        train_dataset,
        validation_data=val_dataset,
        epochs=3
    )

    y_pred = tf.argmax(distilbert_model.predict(test_dataset)[0], axis=1).numpy()
    accuracy = accuracy_score(list(y_test), y_pred)
    fold_accuracies.append(accuracy)

    print(f"Fold {fold} Accuracy: {accuracy:.4f}")
    print(classification_report(list(y_test), y_pred, target_names=['NBC', 'Fox']))

    fold += 1

# 输出平均性能
print("\nK-Fold Cross Validation Results")
print(f"Mean Accuracy: {np.mean(fold_accuracies):.4f}")
print(f"Standard Deviation: {np.std(fold_accuracies):.4f}")

"""***Distil Hyperparameter Tuning***"""

# distil bert - grid search with validation dataset
import tensorflow as tf
from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification, AdamWeightDecay
from sklearn.metrics import classification_report, accuracy_score
import itertools

# 预处理
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

def tokenize_data(texts, tokenizer, max_length=128):
    return tokenizer(
        list(texts),
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_tensors="tf"
    )

# 生成训练集、验证集和测试集
X_train, X_validation, X_test, y_train, y_validation, y_test = train_val_test_gen(df)

# 对训练集、验证集和测试集进行 Tokenization
train_encodings = tokenize_data(X_train, tokenizer)
validation_encodings = tokenize_data(X_validation, tokenizer)
test_encodings = tokenize_data(X_test, tokenizer)

# 创建 TensorFlow Dataset
train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(train_encodings),
    list(y_train)
)).shuffle(len(y_train)).batch(16)

validation_dataset = tf.data.Dataset.from_tensor_slices((
    dict(validation_encodings),
    list(y_validation)
)).batch(16)

test_dataset = tf.data.Dataset.from_tensor_slices((
    dict(test_encodings),
    list(y_test)
)).batch(16)

# 超参数字典
param_grid = {
    'dropout_rate': [0.1,0.3,0.5],
    'epochs': [5, 6,7,8,9],
    'learning_rate': [5e-5]
}

# Grid Search
best_model = None
best_accuracy = 0
best_params = None

# 遍历所有可能的超参数组合
for params in itertools.product(*param_grid.values()):
    current_params = dict(zip(param_grid.keys(), params))
    print(f"Testing params: {current_params}")

    # 创建 DistilBERT 模型
    distilbert_model = TFDistilBertForSequenceClassification.from_pretrained(
        'distilbert-base-uncased',
        num_labels=2
    )

    # 修改 dropout rate
    distilbert_model.config.dropout = current_params['dropout_rate']

    optimizer = AdamWeightDecay(learning_rate=current_params['learning_rate'])
    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
    distilbert_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])

    # EarlyStopping 回调
    early_stopping = tf.keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=10,  # 提前停止的轮次
        restore_best_weights=True
    )

    # 训练模型
    history = distilbert_model.fit(
        train_dataset,
        validation_data=validation_dataset,
        epochs=current_params['epochs'],
        callbacks=[early_stopping]
    )

    # 模型评估
    y_pred = tf.argmax(distilbert_model.predict(test_dataset)[0], axis=1).numpy()
    accuracy = accuracy_score(list(y_test), y_pred)
    print(f"Accuracy: {accuracy:.4f}")

    if accuracy > best_accuracy:
        best_accuracy = accuracy
        best_model = distilbert_model
        best_params = current_params

print(f"Best Accuracy: {best_accuracy:.4f}")
print(f"Best Params: {best_params}")

# 打印 Best Model 的 Classification Report
if best_model:
    y_pred_best = tf.argmax(best_model.predict(test_dataset)[0], axis=1).numpy()
    print("\nClassification Report for Best Model:")
    print(classification_report(list(y_test), y_pred_best, target_names=['NBC', 'Fox']))

# distil bert - grid search
import tensorflow as tf
from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification, AdamWeightDecay
from sklearn.metrics import classification_report, accuracy_score
from sklearn.model_selection import train_test_split
import itertools

# 预处理
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

def tokenize_data(texts, tokenizer, max_length=128):
    return tokenizer(
        list(texts),
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_tensors="tf"
    )

X_train, X_test, y_train, y_test = train_test_gen(df)

train_encodings = tokenize_data(X_train, tokenizer)
test_encodings = tokenize_data(X_test, tokenizer)

train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(train_encodings),
    list(y_train)
)).shuffle(len(y_train)).batch(16)

test_dataset = tf.data.Dataset.from_tensor_slices((
    dict(test_encodings),
    list(y_test)
)).batch(16)

# 超参数字典
param_grid = {
    'dropout_rate': [0.1, 0.3, 0.5],
    'epochs': [3, 5, 7, 10],
    'learning_rate': [5e-5]
}

# Grid Search
best_model = None
best_accuracy = 0
best_params = None

# 遍历所有可能的超参数组合
for params in itertools.product(*param_grid.values()):
    current_params = dict(zip(param_grid.keys(), params))
    print(f"Testing params: {current_params}")

    # 创建 DistilBERT 模型
    distilbert_model = TFDistilBertForSequenceClassification.from_pretrained(
        'distilbert-base-uncased',
        num_labels=2
    )

    # 修改 dropout rate
    distilbert_model.config.dropout = current_params['dropout_rate']

    optimizer = AdamWeightDecay(learning_rate=current_params['learning_rate'])
    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
    distilbert_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])

    # EarlyStopping 回调
    early_stopping = tf.keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=3,  # 提前停止的轮次
        restore_best_weights=True
    )

    # 训练模型
    history = distilbert_model.fit(
        train_dataset,
        validation_data=test_dataset,
        epochs=current_params['epochs'],
        callbacks=[early_stopping]
    )

    # 模型评估
    y_pred = tf.argmax(distilbert_model.predict(test_dataset)[0], axis=1).numpy()
    accuracy = accuracy_score(list(y_test), y_pred)
    print(f"Accuracy: {accuracy:.4f}")

    if accuracy > best_accuracy:
        best_accuracy = accuracy
        best_model = distilbert_model
        best_params = current_params

print(f"Best Accuracy: {best_accuracy:.4f}")
print(f"Best Params: {best_params}")


# 打印 Best Model 的 Classification Report
if best_model:
    y_pred_best = tf.argmax(best_model.predict(test_dataset)[0], axis=1).numpy()
    print("\nClassification Report for Best Model:")
    print(classification_report(list(y_test), y_pred_best, target_names=['NBC', 'Fox']))

"""# Bert Testing"""

# bert
import tensorflow as tf
from transformers import BertTokenizer, TFBertForSequenceClassification
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import AdamWeightDecay # Import AdamWeightDecay

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
X_train, X_test, y_train, y_test = train_test_gen(df)


# Tokenize the text data
def tokenize_data(texts, tokenizer, max_length=128):
    return tokenizer(
        list(texts),
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_tensors="tf"
    )

train_encodings = tokenize_data(X_train, tokenizer)
test_encodings = tokenize_data(X_test, tokenizer)

# Convert to TensorFlow datasets
train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(train_encodings),
    list(y_train)  # Convert labels to list if not already
)).shuffle(len(y_train)).batch(16)

test_dataset = tf.data.Dataset.from_tensor_slices((
    dict(test_encodings),
    list(y_test)  # Convert labels to list if not already
)).batch(16)

# Load the pre-trained BERT model for sequence classification
bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)


# Compile the model
optimizer = AdamWeightDecay(learning_rate=5e-5) # Use AdamWeightDecay
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
bert_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])

# Train the model
history = bert_model.fit(
    train_dataset,
    validation_data=test_dataset,
    epochs=7
)

# Evaluate the model

y_pred = tf.argmax(bert_model.predict(test_dataset)[0], axis=1).numpy()
accuracy = accuracy_score(list(y_test), y_pred)
print(f"Accuracy: {accuracy:.4f}")
print(classification_report(list(y_test), y_pred, target_names=['NBC', 'Fox']))

# test new dataset
df1 = pd.read_csv("/content/drive/My Drive/test_data_random_subset.csv")
print(f"New test file contains {len(df1)} rows.")
X_released_test = df1['title']
y_released_test =df1['labels']

released_test_encodings = tokenize_data(X_released_test, tokenizer)

# 将编码后的数据转为 TensorFlow Dataset
released_test_dataset = tf.data.Dataset.from_tensor_slices((
    dict(released_test_encodings),  # 这里使用新数据的编码
    list(y_released_test)
)).batch(16)

y_pred = tf.argmax(bert_model.predict(released_test_dataset)[0], axis=1).numpy()
accuracy = accuracy_score(list(y_released_test), y_pred)
print(f"Accuracy: {accuracy:.4f}")
print(classification_report(list(y_released_test), y_pred, target_names=['NBC', 'Fox']))

# test new dataset
df1 = pd.read_csv("/content/drive/My Drive/test.csv")
print(f"New test file contains {len(df1)} rows.")
X_released_test = df1['title']
y_released_test =df1['label']

released_test_encodings = tokenize_data(X_released_test, tokenizer)

# 将编码后的数据转为 TensorFlow Dataset
released_test_dataset = tf.data.Dataset.from_tensor_slices((
    dict(released_test_encodings),  # 这里使用新数据的编码
    list(y_released_test)
)).batch(16)

y_pred = tf.argmax(bert_model.predict(released_test_dataset)[0], axis=1).numpy()
accuracy = accuracy_score(list(y_released_test), y_pred)
print(f"Accuracy: {accuracy:.4f}")
print(classification_report(list(y_released_test), y_pred, target_names=['NBC', 'Fox']))

"""# Distil Bert Testing"""

# distilbert
import tensorflow as tf
from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import AdamWeightDecay # Import AdamWeightDecay instead of Keras Adam
# from tensorflow.keras.optimizers import Adam

tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
X_train, X_test, y_train, y_test = train_test_gen(df)

# Tokenize the text data
def tokenize_data(texts, tokenizer, max_length=128):
    return tokenizer(
        list(texts),
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_tensors="tf"
    )

train_encodings = tokenize_data(X_train, tokenizer)
test_encodings = tokenize_data(X_test, tokenizer)

# Convert to TensorFlow datasets
train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(train_encodings),
    list(y_train)  # Convert labels to list if not already
)).shuffle(len(y_train)).batch(16)

test_dataset = tf.data.Dataset.from_tensor_slices((
    dict(test_encodings),
    list(y_test)  # Convert labels to list if not already
)).batch(16)

# Load the pre-trained BERT model for sequence classification
distilbert_model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)


# Compile the model
optimizer = AdamWeightDecay(learning_rate=5e-5) # Use AdamWeightDecay

loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
distilbert_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])

# Train the model
history = distilbert_model.fit(
    train_dataset,
    validation_data=test_dataset,
    epochs=6
)

# Evaluate the model

y_pred = tf.argmax(distilbert_model.predict(test_dataset)[0], axis=1).numpy()
accuracy = accuracy_score(list(y_test), y_pred)
print(f"Accuracy: {accuracy:.4f}")
print(classification_report(list(y_test), y_pred, target_names=['NBC', 'Fox']))

# distilbert with custom dropout rate
import tensorflow as tf
from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, accuracy_score
import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import AdamWeightDecay  # Import AdamWeightDecay instead of Keras Adam

# Tokenizer
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
X_train, X_test, y_train, y_test = train_test_gen(df)

# Tokenize the text data
def tokenize_data(texts, tokenizer, max_length=128):
    return tokenizer(
        list(texts),
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_tensors="tf"
    )

train_encodings = tokenize_data(X_train, tokenizer)
test_encodings = tokenize_data(X_test, tokenizer)

# Convert to TensorFlow datasets
train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(train_encodings),
    list(y_train)  # Convert labels to list if not already
)).shuffle(len(y_train)).batch(16)

test_dataset = tf.data.Dataset.from_tensor_slices((
    dict(test_encodings),
    list(y_test)  # Convert labels to list if not already
)).batch(16)

# Load the pre-trained DistilBERT model for sequence classification
distilbert_model = TFDistilBertForSequenceClassification.from_pretrained(
    'distilbert-base-uncased',
    num_labels=2
)

# Update the dropout rate to 0.3
distilbert_model.config.dropout = 0.3
distilbert_model.config.attention_dropout = 0.3  # Update attention dropout rate if applicable

# Compile the model
optimizer = AdamWeightDecay(learning_rate=5e-5)  # Use AdamWeightDecay

loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
distilbert_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])

# Train the model
history = distilbert_model.fit(
    train_dataset,
    validation_data=test_dataset,
    epochs=7
)

# Evaluate the model
y_pred = tf.argmax(distilbert_model.predict(test_dataset)[0], axis=1).numpy()
accuracy = accuracy_score(list(y_test), y_pred)
print(f"Accuracy: {accuracy:.4f}")
print(classification_report(list(y_test), y_pred, target_names=['NBC', 'Fox']))

# test new dataset
df1 = pd.read_csv("/content/drive/My Drive/test_data_random_subset.csv")
print(f"New test file contains {len(df1)} rows.")
X_released_test = df1['title']
y_released_test =df1['labels']

released_test_encodings = tokenize_data(X_released_test, tokenizer)

# 将编码后的数据转为 TensorFlow Dataset
released_test_dataset = tf.data.Dataset.from_tensor_slices((
    dict(released_test_encodings),  # 这里使用新数据的编码
    list(y_released_test)
)).batch(16)

y_pred = tf.argmax(distilbert_model.predict(released_test_dataset)[0], axis=1).numpy()
accuracy = accuracy_score(list(y_released_test), y_pred)
print(f"Accuracy: {accuracy:.4f}")
print(classification_report(list(y_released_test), y_pred, target_names=['NBC', 'Fox']))

# test new dataset
df1 = pd.read_csv("/content/drive/My Drive/test.csv")
print(f"New test file contains {len(df1)} rows.")
X_released_test = df1['title']
y_released_test =df1['label']

released_test_encodings = tokenize_data(X_released_test, tokenizer)

# 将编码后的数据转为 TensorFlow Dataset
released_test_dataset = tf.data.Dataset.from_tensor_slices((
    dict(released_test_encodings),  # 这里使用新数据的编码
    list(y_released_test)
)).batch(16)

y_pred = tf.argmax(distilbert_model.predict(released_test_dataset)[0], axis=1).numpy()
accuracy = accuracy_score(list(y_released_test), y_pred)
print(f"Accuracy: {accuracy:.4f}")
print(classification_report(list(y_released_test), y_pred, target_names=['NBC', 'Fox']))

"""# Dense NN Testing"""

# adjust initial parameters
from keras.models import Sequential
from keras.layers import Dense, Dropout

X_train, X_test, y_train, y_test = train_test_gen(df)

# Convert the text data to TF-IDF features
vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 3))
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Feed-forward
model = Sequential()
model.add(Dense(1024, input_dim=X_train_tfidf.shape[1], activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.fit(X_train_tfidf, y_train, epochs=10, batch_size=32, validation_data=(X_test_tfidf, y_test))
y_pred = (model.predict(X_test_tfidf) > 0.5).astype(int)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")
print("Classification Report:\n", classification_report(y_test, y_pred))

# add one dense layer
from keras.models import Sequential
from keras.layers import Dense, Dropout

X_train, X_test, y_train, y_test = train_test_gen(df)

# Convert the text data to TF-IDF features
vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 3))
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Feed-forward
model = Sequential()
model.add(Dense(1024, input_dim=X_train_tfidf.shape[1], activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.fit(X_train_tfidf, y_train, epochs=10, batch_size=32, validation_data=(X_test_tfidf, y_test))
y_pred = (model.predict(X_test_tfidf) > 0.5).astype(int)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")
print("Classification Report:\n", classification_report(y_test, y_pred))

df1 = pd.read_csv("/content/drive/My Drive/test_data_random_subset.csv")
print(f"New test file contains {len(df1)} rows.")
X_released_test = df1['title']
y_released_test =df1['labels']

X_released_test_tfidf = vectorizer.transform(X_released_test)
y_pred = (model.predict(X_released_test_tfidf) > 0.5).astype(int)
accuracy = accuracy_score(y_released_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")
print("Classification Report:\n", classification_report(y_released_test, y_pred))

df1 = pd.read_csv("/content/drive/My Drive/test.csv")
print(f"New test file contains {len(df1)} rows.")
X_released_test = df1['title']
y_released_test =df1['label']

X_released_test_tfidf = vectorizer.transform(X_released_test)
y_pred = (model.predict(X_released_test_tfidf) > 0.5).astype(int)
accuracy = accuracy_score(y_released_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")
print("Classification Report:\n", classification_report(y_released_test, y_pred))